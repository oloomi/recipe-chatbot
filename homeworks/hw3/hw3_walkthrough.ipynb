{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: LLM-as-Judge for Recipe Bot Evaluation\n",
    "\n",
    "This notebook walks through building an LLM judge to evaluate dietary adherence.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to label data for judge development\n",
    "- How to write an effective judge prompt\n",
    "- How to measure judge performance (TPR/TNR)\n",
    "- How to use `judgy` to correct for judge bias\n",
    "\n",
    "Video walkthrough: https://youtu.be/1d5aNfslwHg\n",
    "\n",
    "**Bonus**: [Using AI Assisted Coding to Tackle Homework Problems](https://link.courses.maven.com/c/eJw80M2upCAQBeCngZ0Gil8XLGbja5gCymkTbAyoyX37id2Tu6rUl7OoOqm-ajuXLQeQk554qlfr9OxST8rxHHQWMhpOQTrrrFIgNacdt7Kkgr2H2CrmhP38r-fPQYHerZZCmdP7Xr5-XVsOR6t5hKSzIXKDB2MHnQwNHiQMWhIJQ-C9Q_4K3mmbY1zRC_LZw-Scw9XHKCe_Kot8CyDACimMdFIpNRpjwGaX_JogSeuZFt9_-rjjTe8x1Z1vfVlb3ZePhBlLJ17C6zyPztQfBjOD-TfNYD6wFXwnGgrGzmCmG8szQYAZFIO5_5SC8Xpsr_kq9El5J4ziLWwdMY1rwfPFtPj7VPE54w7wLwAA__8a93gB)\n",
    "\n",
    "![AI Assisted Coding Walkthrough Location](../imgs/AIHwWalkthrough.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Look at Your Data First\n",
    "\n",
    "Before writing any code, **look at your data**. We have labeled traces in `reference_files/labeled_traces.jsonl`.\n",
    "\n",
    "> ðŸ’¡ **What's an LLM judge?** Instead of manually reviewing every bot response, we prompt another LLM to do it for us. (No model fine-tuning requiredâ€”just a well-crafted prompt with examples.) But first, we need human-labeled examples to teach the judge what \"good\" and \"bad\" look like.\n",
    "\n",
    "Each trace has:\n",
    "- A user query with a dietary restriction\n",
    "- The bot's response (a recipe)\n",
    "- A human label: PASS or FAIL\n",
    "\n",
    "### Use the HTML Viewer\n",
    "\n",
    "Open `reference_files/trace_viewer.html` in your browser and upload the JSONL file. This lets you:\n",
    "- Navigate between traces with arrow keys\n",
    "- See the query, response, and label for each trace\n",
    "- Understand what PASS vs FAIL looks like\n",
    "\n",
    "**Pro tip**: You can vibe-code your own viewer. Try this prompt:\n",
    "\n",
    "> \"Make a self-contained HTML file to view JSONL files. It should let me upload a file, navigate between records, and display all fields nicely.\"\n",
    "\n",
    "This is a useful skill for quickly exploring any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 101 labeled traces\n",
      "Example keys: ['query', 'dietary_restriction', 'response', 'success', 'error', 'trace_id', 'query_id', 'label', 'reasoning', 'confidence', 'labeled']\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = Path('reference_files')\n",
    "\n",
    "# Load labeled traces (JSONL format)\n",
    "traces = []\n",
    "with open(BASE_PATH / 'labeled_traces.jsonl') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            traces.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(traces)} labeled traces\")\n",
    "print(f\"Example keys: {list(traces[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Gluten-light recipe - I'm not celiac just sensitive\n",
      "Dietary restriction: gluten-free\n",
      "\n",
      "Response (first 400 chars):\n",
      "Absolutely! Here's a delicious and easy Gluten-Light **Lemon Herb Quinoa Salad** perfect for two people.\n",
      "\n",
      "### Ingredients:\n",
      "- 1 cup quinoa\n",
      "- 2 cups water\n",
      "- 1 medium cucumber, diced\n",
      "- 1/4 cup fresh parsley, chopped\n",
      "- 1/4 cup fresh mint, chopped\n",
      "- 1 lemon (for juice and zest)\n",
      "- 2 tablespoons olive oil\n",
      "- Salt and pepper to taste\n",
      "- Optional: 1/4 cup crumbled feta cheese or chopped cherry tomatoes\n",
      "\n",
      "### ...\n",
      "\n",
      "Label: FAIL\n",
      "Reasoning: The recipe is labeled as 'Gluten-Light' and not explicitly 'Gluten-Free'. Quinoa is naturally gluten-free, making it suitable for a gluten-free diet. However, the recipe includes an optional ingredien...\n"
     ]
    }
   ],
   "source": [
    "# Look at one example\n",
    "t = traces[0]\n",
    "print(f\"Query: {t['query']}\")\n",
    "print(f\"Dietary restriction: {t['dietary_restriction']}\")\n",
    "print(f\"\\nResponse (first 400 chars):\\n{t['response'][:400]}...\")\n",
    "print(f\"\\nLabel: {t['label']}\")\n",
    "print(f\"Reasoning: {t['reasoning'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: {'FAIL': 26, 'PASS': 75}\n",
      "\n",
      "Dietary restrictions: {'gluten-free': 10, 'nut-free': 2, 'vegan': 11, 'kosher': 2, 'paleo': 10, 'dairy-free': 4, 'low-carb': 11, 'raw vegan': 4, 'keto': 7, 'pescatarian': 6, 'whole30': 2, 'sugar-free': 4, 'vegetarian': 18, 'halal': 3, 'diabetic-friendly': 6, 'low-sodium': 1}\n"
     ]
    }
   ],
   "source": [
    "# Check label distribution\n",
    "labels = Counter(t['label'] for t in traces)\n",
    "print(f\"Label distribution: {dict(labels)}\")\n",
    "\n",
    "# Check dietary restriction distribution\n",
    "restrictions = Counter(t['dietary_restriction'] for t in traces)\n",
    "print(f\"\\nDietary restrictions: {dict(restrictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the Data\n",
    "\n",
    "> ðŸ’¡ **Why split?** Train examples teach the judge what to look for. Dev lets us iterate and improve. Test is our final check that the solution generalizesâ€”we hold it out until the end to ensure we haven't overfit to our training and dev sets.\n",
    "\n",
    "We split into:\n",
    "- **Train (10-20%)**: For few-shot examples in the judge prompt\n",
    "- **Dev (40%)**: For iterating on the prompt\n",
    "- **Test (40-50%)**: For final evaluation (don't peek until the end!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 15 traces\n",
      "Dev: 40 traces\n",
      "Test: 46 traces\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "shuffled = traces.copy()\n",
    "random.shuffle(shuffled)\n",
    "\n",
    "n = len(shuffled)\n",
    "train_end = int(n * 0.15)\n",
    "dev_end = int(n * 0.55)\n",
    "\n",
    "train_set = shuffled[:train_end]\n",
    "dev_set = shuffled[train_end:dev_end]\n",
    "test_set = shuffled[dev_end:]\n",
    "\n",
    "print(f\"Train: {len(train_set)} traces\")\n",
    "print(f\"Dev: {len(dev_set)} traces\")\n",
    "print(f\"Test: {len(test_set)} traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'PASS': 11, 'FAIL': 4}\n",
      "Dev: {'PASS': 33, 'FAIL': 7}\n",
      "Test: {'PASS': 31, 'FAIL': 15}\n"
     ]
    }
   ],
   "source": [
    "# Check label balance in each split\n",
    "for name, split in [(\"Train\", train_set), (\"Dev\", dev_set), (\"Test\", test_set)]:\n",
    "    counts = Counter(t['label'] for t in split)\n",
    "    print(f\"{name}: {dict(counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Judge Prompt\n",
    "\n",
    "Now we write the instructions that tell our LLM judge how to evaluate responses. We need to explain the task clearly and show examples of what PASS and FAIL look like.\n",
    "\n",
    "A good judge prompt includes:\n",
    "1. The task and criterion\n",
    "2. Clear PASS/FAIL definitions\n",
    "3. Few-shot examples from the train set\n",
    "4. Expected output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected examples for few-shot:\n",
      "\n",
      "PASS example: Keto meal that doesn't make me feel gross...\n",
      "FAIL example: I want something light but filling...\n"
     ]
    }
   ],
   "source": [
    "# Select few-shot examples (one PASS, one FAIL)\n",
    "pass_examples = [t for t in train_set if t['label'] == 'PASS']\n",
    "fail_examples = [t for t in train_set if t['label'] == 'FAIL']\n",
    "\n",
    "example_pass = pass_examples[0] if pass_examples else None\n",
    "example_fail = fail_examples[0] if fail_examples else None\n",
    "\n",
    "print(\"Selected examples for few-shot:\")\n",
    "if example_pass:\n",
    "    print(f\"\\nPASS example: {example_pass['query'][:60]}...\")\n",
    "if example_fail:\n",
    "    print(f\"FAIL example: {example_fail['query'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge prompt template (first 1500 chars):\n",
      "\n",
      "You are evaluating whether a recipe chatbot correctly followed a user's dietary restriction.\n",
      "\n",
      "## Criterion: Dietary Adherence\n",
      "\n",
      "The bot should provide recipes that actually meet the user's stated dietary restrictions.\n",
      "\n",
      "- **PASS**: The recipe fully adheres to the dietary restriction. All ingredients are compliant.\n",
      "- **FAIL**: The recipe violates the dietary restriction in any way (wrong ingredients, cross-contamination risk, etc.)\n",
      "\n",
      "## Common Dietary Restrictions\n",
      "\n",
      "- Vegan: No animal products (meat, dairy, eggs, honey)\n",
      "- Vegetarian: No meat or fish, but dairy and eggs allowed\n",
      "- Gluten-free: No wheat, barley, rye, or gluten-containing ingredients\n",
      "- Keto: Very low carb (<20g net carbs), high fat\n",
      "- Nut-free: No tree nuts or peanuts\n",
      "\n",
      "## Examples\n",
      "\n",
      "\n",
      "### Example 1 (PASS)\n",
      "\n",
      "Query: Keto meal that doesn't make me feel gross\n",
      "Dietary Restriction: keto\n",
      "Response: Absolutely! I recommend trying a **Garlic Butter Shrimp with Spinach**. It's flavorful, satisfying, and keto-friendly, with a lovely balance of richness and freshnessâ€”perfect to avoid that \"gross\" feeling. This recipe serves 2 people.\n",
      "\n",
      "### Ingredients:\n",
      "- 1/2 pound (250g) large shrimp, peeled and deveined\n",
      "- 4 cups fresh spinach\n",
      "- 3 tablespoons unsalted butter\n",
      "- 3 cloves garlic, minced\n",
      "- 1 teaspoon paprika (optional, for flavor)\n",
      "- Salt and freshly ground black pepper, to taste\n",
      "- 1 tablespoon olive...\n",
      "\n",
      "Judgment: PASS\n",
      "Reasoning: The recipe for Garlic Butter Shrimp with Spinach adheres to the keto dietary restriction. It includes shrimp, s\n"
     ]
    }
   ],
   "source": [
    "# Build the judge prompt\n",
    "def build_judge_prompt(example_pass, example_fail):\n",
    "    prompt = \"\"\"\n",
    "You are evaluating whether a recipe chatbot correctly followed a user's dietary restriction.\n",
    "\n",
    "## Criterion: Dietary Adherence\n",
    "\n",
    "The bot should provide recipes that actually meet the user's stated dietary restrictions.\n",
    "\n",
    "- **PASS**: The recipe fully adheres to the dietary restriction. All ingredients are compliant.\n",
    "- **FAIL**: The recipe violates the dietary restriction in any way (wrong ingredients, cross-contamination risk, etc.)\n",
    "\n",
    "## Common Dietary Restrictions\n",
    "\n",
    "- Vegan: No animal products (meat, dairy, eggs, honey)\n",
    "- Vegetarian: No meat or fish, but dairy and eggs allowed\n",
    "- Gluten-free: No wheat, barley, rye, or gluten-containing ingredients\n",
    "- Keto: Very low carb (<20g net carbs), high fat\n",
    "- Nut-free: No tree nuts or peanuts\n",
    "\n",
    "## Examples\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    if example_pass:\n",
    "        prompt += f\"\"\"\n",
    "### Example 1 (PASS)\n",
    "\n",
    "Query: {example_pass['query']}\n",
    "Dietary Restriction: {example_pass['dietary_restriction']}\n",
    "Response: {example_pass['response'][:500]}...\n",
    "\n",
    "Judgment: PASS\n",
    "Reasoning: {example_pass['reasoning'][:200]}...\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    if example_fail:\n",
    "        prompt += f\"\"\"\n",
    "### Example 2 (FAIL)\n",
    "\n",
    "Query: {example_fail['query']}\n",
    "Dietary Restriction: {example_fail['dietary_restriction']}\n",
    "Response: {example_fail['response'][:500]}...\n",
    "\n",
    "Judgment: FAIL\n",
    "Reasoning: {example_fail['reasoning'][:200]}...\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "## Your Task\n",
    "\n",
    "Evaluate the following trace. Respond with JSON:\n",
    "{\"judgment\": \"PASS\" or \"FAIL\", \"reasoning\": \"your reasoning here\"}\n",
    "\n",
    "Query: {query}\n",
    "Dietary Restriction: {dietary_restriction}\n",
    "Response: {response}\n",
    "\n",
    "Judgment:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "judge_prompt = build_judge_prompt(example_pass, example_fail)\n",
    "print(\"Judge prompt template (first 1500 chars):\")\n",
    "print(judge_prompt[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate on Dev Set\n",
    "\n",
    "To actually run the judge, you'd call an LLM API. Here we'll simulate the process.\n",
    "\n",
    "> ðŸ’¡ **Why measure both TPR and TNR?** A judge that says \"PASS\" to everything has perfect TPR but zero TNRâ€”it catches no failures! We need both metrics to know the judge is actually useful.\n",
    "\n",
    "Key metrics:\n",
    "- **TPR (True Positive Rate)**: Of actual PASSes, how many did we correctly identify?\n",
    "- **TNR (True Negative Rate)**: Of actual FAILs, how many did we correctly identify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran judge on 40 dev traces\n"
     ]
    }
   ],
   "source": [
    "# Simulated judge predictions (in practice, you'd call an LLM)\n",
    "# For demonstration, we'll assume the judge matches ground truth 85% of the time\n",
    "def simulate_judge(trace, accuracy=0.85):\n",
    "    \"\"\"Simulate a judge that's correct 85% of the time.\"\"\"\n",
    "    if random.random() < accuracy:\n",
    "        return trace['label']  # Correct\n",
    "    else:\n",
    "        return 'FAIL' if trace['label'] == 'PASS' else 'PASS'  # Wrong\n",
    "\n",
    "# Run on dev set\n",
    "random.seed(123)\n",
    "dev_predictions = [(t['label'], simulate_judge(t)) for t in dev_set]\n",
    "\n",
    "print(f\"Ran judge on {len(dev_predictions)} dev traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set Metrics\n",
      "==============================\n",
      "TPR (True Positive Rate): 87.9%\n",
      "TNR (True Negative Rate): 100.0%\n",
      "\n",
      "Confusion matrix:\n",
      "  TP=29, FN=4\n",
      "  FP=0, TN=7\n"
     ]
    }
   ],
   "source": [
    "# Calculate TPR and TNR\n",
    "def calculate_metrics(predictions):\n",
    "    \"\"\"Calculate TPR and TNR from (actual, predicted) pairs.\"\"\"\n",
    "    tp = sum(1 for actual, pred in predictions if actual == 'PASS' and pred == 'PASS')\n",
    "    fn = sum(1 for actual, pred in predictions if actual == 'PASS' and pred == 'FAIL')\n",
    "    tn = sum(1 for actual, pred in predictions if actual == 'FAIL' and pred == 'FAIL')\n",
    "    fp = sum(1 for actual, pred in predictions if actual == 'FAIL' and pred == 'PASS')\n",
    "    \n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'TPR': tpr,\n",
    "        'TNR': tnr,\n",
    "        'TP': tp, 'FN': fn, 'TN': tn, 'FP': fp\n",
    "    }\n",
    "\n",
    "dev_metrics = calculate_metrics(dev_predictions)\n",
    "print(\"Dev Set Metrics\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"TPR (True Positive Rate): {dev_metrics['TPR']:.1%}\")\n",
    "print(f\"TNR (True Negative Rate): {dev_metrics['TNR']:.1%}\")\n",
    "print(f\"\\nConfusion matrix:\")\n",
    "print(f\"  TP={dev_metrics['TP']}, FN={dev_metrics['FN']}\")\n",
    "print(f\"  FP={dev_metrics['FP']}, TN={dev_metrics['TN']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use Judgy to Correct Bias\n",
    "\n",
    "Since no judge is perfect, we use math to correct for its mistakes. If we know a judge has 90% TPR and 85% TNR, we can work backwards from its predictions to estimate the *true* success rate.\n",
    "\n",
    "> ðŸ’¡ **The key insight**: A biased judge gives biased resultsâ€”but if we know *how* it's biased (from our dev set evaluation), we can mathematically adjust for it.\n",
    "\n",
    "The `judgy` library does this correction for you.\n",
    "\n",
    "Install: `pip install judgy`\n",
    "\n",
    "Documentation: https://github.com/ai-evals-course/judgy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example judgy output (simulated):\n",
      "========================================\n",
      "Raw Observed Success Rate: 0.857 (85.7%)\n",
      "Corrected Success Rate: 0.926 (92.6%)\n",
      "95% Confidence Interval: [0.817, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Example of using judgy (uncomment to run if installed)\n",
    "# from judgy import estimate_success_rate\n",
    "#\n",
    "# # Convert labels and predictions to binary (1=PASS, 0=FAIL)\n",
    "# test_labels = [1 if actual == 'PASS' else 0 for actual, _ in dev_predictions]\n",
    "# test_preds = [1 if pred == 'PASS' else 0 for _, pred in dev_predictions]\n",
    "#\n",
    "# # unlabeled_preds would be your judge's predictions on unlabeled data\n",
    "# # For demonstration, we reuse test_preds\n",
    "# unlabeled_preds = test_preds\n",
    "#\n",
    "# theta_hat, lower, upper = estimate_success_rate(test_labels, test_preds, unlabeled_preds)\n",
    "# raw_pass_rate = sum(test_preds) / len(test_preds)\n",
    "# print(f\"Raw observed pass rate: {raw_pass_rate:.1%}\")\n",
    "# print(f\"Corrected pass rate: {theta_hat:.1%}\")\n",
    "# print(f\"95% Confidence Interval: [{lower:.3f}, {upper:.3f}]\")\n",
    "\n",
    "# Simulated example\n",
    "print(\"Example judgy output (simulated):\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Raw Observed Success Rate: 0.857 (85.7%)\")\n",
    "print(\"Corrected Success Rate: 0.926 (92.6%)\")\n",
    "print(\"95% Confidence Interval: [0.817, 1.000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation on Test Set\n",
    "\n",
    "Once you're happy with your judge, run it on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Metrics (Final)\n",
      "==============================\n",
      "TPR (True Positive Rate): 77.4%\n",
      "TNR (True Negative Rate): 86.7%\n"
     ]
    }
   ],
   "source": [
    "# Run on test set\n",
    "random.seed(456)\n",
    "test_predictions = [(t['label'], simulate_judge(t)) for t in test_set]\n",
    "\n",
    "test_metrics = calculate_metrics(test_predictions)\n",
    "print(\"Test Set Metrics (Final)\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"TPR (True Positive Rate): {test_metrics['TPR']:.1%}\")\n",
    "print(f\"TNR (True Negative Rate): {test_metrics['TNR']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we covered:**\n",
    "1. Loading and understanding labeled traces\n",
    "2. Splitting data into train/dev/test\n",
    "3. Building a judge prompt with few-shot examples\n",
    "4. Measuring TPR and TNR\n",
    "5. Using judgy to correct for bias\n",
    "\n",
    "**Your deliverables:**\n",
    "1. Labeled dataset with train/dev/test splits\n",
    "2. Final judge prompt with few-shot examples\n",
    "3. Judge performance (TPR/TNR on test set)\n",
    "4. Final evaluation using judgy (raw rate, corrected rate, CI)\n",
    "5. Brief analysis (1-2 paragraphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
